{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huber Regression\n",
    "\n",
    "$$\\text{minimize  }  \\displaystyle\\sum_{i=1}^m\\phi(a_i^Tx-b_i) $$\n",
    "\n",
    "where $\\phi$ is the Huber penalty function and $a_i^T$ is the $i$th row of the given data matrix.\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\left\\{\n",
    "     \\begin{array}{lr}\n",
    "     \tu^2 &  \\text{if } |u| \\leq \\epsilon  \\\\\n",
    "       \\epsilon(2|u|-\\epsilon) & u > \\epsilon\n",
    "     \\end{array}\n",
    "   \\right.\n",
    "$$\n",
    "\n",
    "for some constant $M$. The objective function is differentiable, but it is not twice continuously differentiable. So, we can use the gradient descent method, but we cannot use Newton's method. Instead, we will implement a quasi-Newton method.\n",
    "\n",
    "We introduce a scale parameter $\\sigma$ and introduce some regularization and instead solve\n",
    "\n",
    "$$\\text{minimize  }  \\displaystyle\\sum_{i=1}^m\\left(\\sigma+ \\phi\\left(\\frac{a_i^Tx-b_i}{\\sigma}\\right) \\sigma\\right) + \\gamma \\|x\\|_2^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some test data\n",
    "m=200\n",
    "n=10\n",
    "A = np.random.normal(0,1,(m,n))\n",
    "real_weights = np.random.rand(10)\n",
    "b = A@real_weights + np.random.normal(0,0.25, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We need to compute the gradient to implement the gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(x):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "def huber(x, eps):\n",
    "    return np.where(np.abs(x)<=eps, x**2, 2*np.abs(x)*eps-eps**2)\n",
    "def objective(x, A ,b, sigma,eps,gamma):\n",
    "    return np.sum(sigma+huber((A@x-b)/sigma,eps)*sigma) + gamma*l2_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_prime(x, eps):\n",
    "    return np.where(np.abs(x)<=eps, 2*x, 2*eps*np.sign(x))\n",
    "# last entry of vector is for variable sigma\n",
    "def grad(x, A, b, sigma, eps, gamma):\n",
    "    dx = A.T@huber_prime((A@x-b)/sigma, eps) + 2*gamma*x\n",
    "    d_sigma = A.shape[0] + np.sum(huber((A@x-b)/sigma,eps)) - huber_prime((A@x-b)/sigma, eps)@((A@x-b)/sigma)\n",
    "    return np.append(dx,d_sigma)\n",
    "def neg_grad(x, A, b, sigma, eps, gamma):\n",
    "    return -grad(x, A, b, sigma, eps, gamma)\n",
    "# line search\n",
    "def GD_backtrack(obj, x, A, b, sigma, eps, gamma, gradient, alpha, beta):\n",
    "    t = 1\n",
    "    # we need to make sure the scale parameter is >= 0\n",
    "    while sigma-t*gradient[-1]<=0:\n",
    "        t = t*beta\n",
    "    while obj(x-t*gradient[:-1], A, b, sigma-t*gradient[-1], eps, gamma) > obj(x, A ,b, sigma,eps,gamma) - alpha*t*gradient@gradient:\n",
    "        t = t*beta\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "x = np.zeros(n) # initial point\n",
    "sigma = 1.0\n",
    "tol = 1e-5 # stopping criterion tolerance for L2 norm of gradient\n",
    "max_iter = 1000\n",
    "# parameters \n",
    "eps = 1.35 # huber penalty\n",
    "gamma = 0.0001 # regularization\n",
    "alpha = 0.1 # line search\n",
    "beta = 0.5 # line search\n",
    "i = 0\n",
    "while i <= max_iter:\n",
    "    g = grad(x, A, b, sigma, eps, gamma)\n",
    "    if l2_norm(g) < tol:\n",
    "        break\n",
    "    else:\n",
    "        t = GD_backtrack(objective, x, A, b, sigma, eps, gamma, g, alpha, beta) # step length\n",
    "        x = x - t*g[:-1] # update\n",
    "        sigma = sigma - t*g[-1]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90556862, 0.70739432, 0.23543601, 1.02409131, 0.58045302,\n",
       "       0.06581826, 0.46918598, 0.83084905, 0.58434635, 0.75306095])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1521417045596456"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90580018, 0.70743163, 0.23612307, 1.02418985, 0.58097797,\n",
       "       0.06709052, 0.46905683, 0.83119728, 0.58406313, 0.75214576])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr = HuberRegressor().fit(A,b)\n",
    "hr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15186606514984424"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Method\n",
    "\n",
    "Since the huber penalty function is not twice continuously differentiable, we cannot implement Newton's Method. Instead, we use a Quasi-Newton method. In particular, we will follow the BFGS algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QN_backtrack(obj, x, A, b, sigma, eps, gamma, gradient, step, alpha, beta):\n",
    "    t = 1\n",
    "    # we need to make sure the scale parameter is >= 0\n",
    "    while sigma+t*step[-1]<=0:\n",
    "        t = t*beta\n",
    "    while obj(x+t*step[:-1], A, b, sigma+t*step[-1], eps, gamma) > obj(x, A ,b, sigma,eps,gamma) + alpha*t*gradient@step:\n",
    "        t = t*beta\n",
    "    return t\n",
    "def bfgs(H, s, y):\n",
    "    return H + (y@y.T)/(y.T@s)- (H@s@s.T@H)/(s.T@(H@s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quasi-Netwon Method\n",
    "x = np.zeros(n) # initial point\n",
    "sigma = 1.0\n",
    "H = np.eye(n+1)\n",
    "tol = 1e-5 \n",
    "max_iter = 100\n",
    "# parameters \n",
    "eps = 1.35 # huber penalty\n",
    "gamma = 0.0001 # regularization\n",
    "alpha = 0.1 # line search\n",
    "beta = 0.5 # line search\n",
    "s, y = np.append(x,sigma), grad(x, A, b, sigma, eps, gamma)\n",
    "g = grad(x, A, b, sigma, eps, gamma)\n",
    "i = 0\n",
    "diff = np.inf\n",
    "while i <= max_iter: # max number of iterations as stopping criterion\n",
    "    if diff < tol: # stop when the difference between consecutive objective values is \"small\"\n",
    "        break\n",
    "    else:\n",
    "        diff = objective(x, A ,b, sigma,eps,gamma)\n",
    "        # quasi-newton direction\n",
    "        qn_step = -np.linalg.inv(H)@g\n",
    "        # line search for step length\n",
    "        t = QN_backtrack(objective, x, A, b, sigma, eps, gamma, g, qn_step, alpha, beta)\n",
    "        # update\n",
    "        x = x+t*qn_step[:-1]\n",
    "        sigma = sigma +t*qn_step[-1]\n",
    "        # set up for bfgs update of Hessian\n",
    "        diff = np.abs(objective(x, A ,b, sigma,eps,gamma) - diff)\n",
    "        g = grad(x, A, b, sigma, eps, gamma)\n",
    "        w = np.append(x,sigma)\n",
    "        x_diff = w - s\n",
    "        grad_diff = g - y\n",
    "        grad_diff = grad_diff.reshape(n+1,1)\n",
    "        x_diff = x_diff.reshape(n+1,1)\n",
    "        H = bfgs(H,x_diff,grad_diff)\n",
    "        s = w\n",
    "        y = g\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90556169, 0.70739524, 0.23543639, 1.02408851, 0.58044768,\n",
       "       0.06581175, 0.46918461, 0.83084089, 0.58434494, 0.75306439])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15215112019498284"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
