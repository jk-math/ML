{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equality Constrained Entropy Maximization\n",
    "\n",
    "\n",
    "This is an exercise from [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/)\n",
    "\n",
    "$$ \\text{minimize } \\qquad f(x) = \\displaystyle\\sum_{i=1}^n x_i\\log(x_i) $$\n",
    "\n",
    "$$ \\text{subject to } \\qquad Ax=b $$\n",
    "\n",
    "## Feasible Start\n",
    "\n",
    "For now, we assume that we have a feasible point $x$, so we assume that we have a point $x$ such that $Ax=b$.\n",
    "\n",
    "Let's replace the objective with its second order Taylor Approximation at $x$.\n",
    "\n",
    "$$ \\hat{f}(v) = f(x) + \\nabla f(x)^T (v-x) + \\frac{1}{2} (v-x)^T \\nabla^2 f(x) (v-x) $$\n",
    "\n",
    "$$ \\text{minimize } \\qquad \\hat{f}(x+v) = \\frac{1}{2} v^T \\nabla^2 f(x) v + \\nabla f(x)^Tv + f(x)$$\n",
    "\n",
    "$$ \\text{subject to} \\qquad A(x+v)=b$$\n",
    "\n",
    "with respect to the variable $v$. We can solve this problem and we think of the solution as the value which needs to be added to $x$ to minimize the quadratic approximation. This value is the Newton step.\n",
    "\n",
    "We can solve the quadratic approximation minimization problem (and therefore find the Newton step) by forming the dual and solving an unconstrained minimization problem.\n",
    "\n",
    "$$ \\text{minimize } \\qquad \\frac{1}{2} v^T \\nabla^2 f(x) v + \\nabla f(x)^T v + f(x) + \\lambda^T(A(x+v)-b) $$\n",
    "\n",
    "A solution to this problem is equivalent to finding a solution $v^{\\star}, \\lambda^{\\star}$ to the KKT equations\n",
    "\n",
    "$$A(x+v^{\\star})=b \\qquad \\text{ and } \\qquad \\nabla^2 f(x)v^{\\star} + \\nabla f(x) + A^T \\lambda^{\\star} = 0$$\n",
    "\n",
    "Note that we compute the gradients with respect to the variable $v$. \n",
    "\n",
    "We re-write the equations\n",
    "\n",
    "$$Av^{\\star}=b-Ax = 0 \\qquad \\text{ and } \\qquad \\nabla^2 f(x)v^{\\star} +  A^T \\lambda^{\\star} = -\\nabla f(x)$$\n",
    "\n",
    "\n",
    "We can compute the Newton step by solving the KKT system\n",
    "\n",
    "$$ \\begin{bmatrix} \\nabla^2f(x) & A^T \\\\ A & 0 \\end{bmatrix} \\begin{bmatrix} v^{\\star} \\\\ \\lambda^{\\star} \\end{bmatrix}  = \\begin{bmatrix} -\\nabla f(x) \\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "where $\\Delta x_{nt} = v^{\\star}$ is the Newton step (the step we use to update our current feasible point) and $w = \\lambda^{\\star}$ is the updated dual variable (already updated, no step necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate problem data\n",
    "n = 100\n",
    "p = 30\n",
    "A = np.random.normal(0,1,(p,n))\n",
    "x_hat = np.random.uniform(0,10,100) # use this as feasible intial point\n",
    "b = A@x_hat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve KKT system via block elimination\n",
    "def KKT_solve(H,A,g,h):\n",
    "    H_inv = np.linalg.inv(H)\n",
    "    X = H_inv@A.T\n",
    "    y = H_inv@g\n",
    "    S_inv = np.linalg.inv(-A@X)\n",
    "    w = S_inv@(A@y-h)\n",
    "    return H_inv@(-A.T@w-g), w\n",
    "def l2_norm(x):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "def entropy(x):\n",
    "    return np.sum(x*np.log(x))\n",
    "def grad(x):\n",
    "    return np.log(x)+1\n",
    "def hessian(x):\n",
    "    return np.diag(1/x)\n",
    "def backtrack(x, objective, alpha, beta, grad, descent_direction):\n",
    "    t = 1\n",
    "    while np.min(x+t*descent_direction)<=0: # check that updated point will be in the domain\n",
    "        t = t*beta\n",
    "    while objective(x+t*descent_direction) > objective(x) + alpha*t*(grad@descent_direction):\n",
    "        t = t*beta\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Newtons_method(obj, A, b, x_init, grad_func, hessian_func, tol=1e-8, max_iter=1000, alpha=0.1, beta=0.5 ):\n",
    "    x = x_init\n",
    "    i = 0\n",
    "    if (A@x-b == 0).all():\n",
    "        print('Feasible start')\n",
    "        while i <= max_iter:\n",
    "            g = grad_func(x)\n",
    "            H = hessian_func(x)\n",
    "            delta_x, w = KKT_solve(H,A,g,np.zeros(A.shape[0]))\n",
    "            dec = delta_x@(H@delta_x)\n",
    "            if dec < tol:\n",
    "                break\n",
    "            else:\n",
    "                t = backtrack(x, obj, alpha, beta, g, delta_x)\n",
    "                x = x+t*delta_x\n",
    "            i += 1\n",
    "    else:\n",
    "        print('Infeasible Start')\n",
    "        w = np.zeros(A.shape[0])\n",
    "        while i <= max_iter:\n",
    "            g = grad_func(x)\n",
    "            H = hessian_func(x)\n",
    "            delta_x, updated_w = KKT_solve(H,A,g,A@x-b)\n",
    "            delta_w = updated_w - w\n",
    "            t = r_backtrack(x,w,delta_x,delta_w,A,b, alpha, beta)\n",
    "            x = x+t*delta_x\n",
    "            w = w+t*delta_w\n",
    "            if l2_norm(A@x-b) < tol and l2_norm(r(x,w,A,b))< tol:\n",
    "                break\n",
    "            i += 1\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feasible start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "218.99175125160755"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feasible start example\n",
    "solution = Newtons_method(entropy, A,b, x_hat, grad, hessian)\n",
    "entropy(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infeasible Start\n",
    "\n",
    "Previously, we have seen how to find the Newton step by solving the KKT equations\n",
    "\n",
    "$$Av^{\\star}=b-Ax = 0 \\qquad \\text{ and } \\qquad \\nabla^2 f(x)v^{\\star} +  A^T \\lambda^{\\star} = -\\nabla f(x)$$\n",
    "\n",
    "Note that if the point $x$ is infeasible, $Ax-b \\neq 0$. So the KKT system becomes\n",
    "\n",
    "$$ \\begin{bmatrix} \\nabla^2f(x) & A^T \\\\ A & 0 \\end{bmatrix} \\begin{bmatrix} v^{\\star} \\\\ \\lambda^{\\star} \\end{bmatrix}  = -\\begin{bmatrix} \\nabla f(x) \\\\ Ax-b \\end{bmatrix}$$\n",
    "\n",
    "We want to update the dual variable as well to approximately satisfy the optimality conditions. From the KKT conditions for the original problem, we want \n",
    "\n",
    "$$ r_{\\text{dual}} (x, \\lambda) = \\nabla f(x) + A^T \\lambda \\qquad \\qquad r_{\\text{primal}}(x, \\lambda) = Ax-b$$\n",
    "\n",
    "the dual and primal residuals to be 0. So, we want to update the primal and dual variables to drive the entries of \n",
    "\n",
    "$$ r(x ,\\lambda) = (r_{\\text{dual}} (x, \\lambda),r_{\\text{primal}}(x, \\lambda)) $$\n",
    "\n",
    "to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(x,w,A,b):\n",
    "    return np.append(grad(x)+A.T@w, A@x-b)\n",
    "def r_backtrack(x,w,delta_x,delta_w,A,b, alpha, beta):\n",
    "    t = 1\n",
    "    l2 = l2_norm(r(x,w,A,b))\n",
    "    while np.min(x+t*delta_x) <= 0: # the gradient involves log, need to have updated point in domain\n",
    "        t = t*beta\n",
    "    while l2_norm(r(x+t*delta_x, w+t*delta_w,A,b)) > (1-alpha*t)*l2:\n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infeasible Start\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "218.99175125160863"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(Newtons_method(entropy, A, b, np.ones(A.shape[1]),grad, hessian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218.991750660826"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cp.Variable(n)\n",
    "objective = cp.Minimize(cp.sum(-1*cp.entr(x)))\n",
    "constraints = [A@x==b]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "prob.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the dual of the original problem\n",
    "\n",
    "Original Problem\n",
    "\n",
    "$$ \\text{minimize } \\qquad f(x) = \\displaystyle\\sum_{i=1}^n x_i\\log(x_i) $$\n",
    "\n",
    "$$ \\text{subject to } \\qquad Ax=b $$\n",
    "\n",
    "Dual Problem\n",
    "\n",
    "$$ \\text{maximize } \\qquad g(\\lambda) = \\text{inf} \\left( f(x) + \\lambda^T(Ax-b)\\right)$$\n",
    "\n",
    "We can express the dual function $g$ using the conjugate $f^{*}$ of the objective function $f$:\n",
    "\n",
    "$$g(\\lambda) = -b^T\\lambda - f^{*} (-A^T \\lambda)$$\n",
    "\n",
    "The conjugate of negative entropy is \n",
    "\n",
    "$$ f^{*} (y) = \\displaystyle\\sum_{i=1}^n \\text{exp}(y_i-1) $$\n",
    "\n",
    "Our goal is to solve the problem\n",
    "\n",
    "$$ \\text{maximize} \\qquad -b^T\\lambda - \\displaystyle\\sum_{i=1}^n \\text{exp}(-a_i^T\\lambda - 1)$$\n",
    "\n",
    "where $a_i$ is the $i$th column of the matrix $A$. We solve the equivalent problem\n",
    "\n",
    "$$ \\text{minimize} \\qquad b^T\\lambda + \\displaystyle\\sum_{i=1}^n \\text{exp}(-a_i^T\\lambda - 1)$$\n",
    "\n",
    "with respect to the variable $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_backtrack(x, objective,A, b, alpha, beta, gradient, descent_direction):\n",
    "    t = 1\n",
    "    while objective(x+t*descent_direction, A, b) > objective(x,A,b)+alpha*t*gradient@descent_direction:\n",
    "        t = t*beta\n",
    "    return t\n",
    "def dual_grad(x,A,b):\n",
    "    return b-A@(np.exp(-A.T@x-1))\n",
    "def dual_hessian(x, A, b):\n",
    "    return A@np.diag(np.exp(-A.T@x-1))@A.T\n",
    "def dual_objective(x,A,b):\n",
    "    return b@x + np.sum(np.exp(-A.T@x-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method\n",
    "z = np.zeros(A.shape[0])\n",
    "tol = 1e-8 \n",
    "max_iter = 100\n",
    "alpha = 0.1 # line search\n",
    "beta = 0.5 # line search\n",
    "i = 0\n",
    "while i <= max_iter:\n",
    "    g = dual_grad(z, A, b)\n",
    "    H = dual_hessian(z, A, b)\n",
    "    newton_step = -np.linalg.inv(H)@g\n",
    "    dec = -g@newton_step\n",
    "    if dec < tol:\n",
    "        break\n",
    "    else:\n",
    "        t = dual_backtrack(z, dual_objective,A,b,alpha, beta, g, newton_step)\n",
    "        z = z+t*newton_step\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218.9917512515649"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-dual_objective(z,A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we find a solution $\\lambda^*$ to the dual problem. Then, if we can find a point $x^*$ which minimizes the Lagrangian $L(x,\\lambda^*)$, then $x^*$ is a primal optimal point. \n",
    "\n",
    "$$L(x, \\lambda^*) = \\displaystyle\\sum_{i=1}^n x_i\\log(x_i) + A^T \\lambda^* $$\n",
    "\n",
    "We can compute the gradient, set its entries equal to 0, and solve to find an optimal primal point.\n",
    "\n",
    "$$ [\\nabla L(x, \\lambda^*)]_i = \\log(x_i) + 1 + a_i^T\\lambda^* = 0 $$\n",
    "\n",
    "$$ x_i = \\text{exp}(-a_i^T-1) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.34334804,  0.09825093,  1.41879333,  2.09632328,  2.12707902,\n",
       "        1.79612066,  3.04164402,  1.0391971 ,  1.58946936,  0.02585689,\n",
       "        0.33200857,  0.65157311,  0.40973208,  0.44675065,  4.04244539,\n",
       "        2.6315314 ,  0.05165718,  1.27951011,  7.45419904,  1.24650019,\n",
       "        0.20762563,  0.91567377,  3.12948152,  3.11113851,  4.93902936,\n",
       "        1.44618547, 12.77851283,  3.52911626, 11.02331712,  2.61983735,\n",
       "        4.02838263,  0.16815896,  0.59512935,  0.52279157,  0.4849513 ,\n",
       "        0.65701703,  2.14008806,  7.47662646,  1.38770218,  0.1382485 ,\n",
       "        0.05580886,  0.30922579,  0.04741908,  0.51898547,  0.92303269,\n",
       "        0.54110112,  0.27637043,  1.1709028 ,  3.63364308,  1.41175495,\n",
       "        6.20678069,  0.83365602,  0.02542982,  1.89561086,  4.5765728 ,\n",
       "        3.01566282,  0.36505276,  0.05132818,  0.81037054,  2.35607558,\n",
       "        2.05845361,  2.12914127,  0.08766997,  1.15716089,  0.03357572,\n",
       "        1.35149981,  0.26762997,  3.70224108,  0.18464198,  0.8014854 ,\n",
       "        0.76439946,  0.16677738,  0.19331097,  1.93859242,  0.77786257,\n",
       "        0.71544588,  2.07264457,  3.79026031,  0.6734631 ,  0.06286882,\n",
       "        0.13383945,  3.03311271,  0.56619941,  2.13544034,  0.05061591,\n",
       "        0.37151496,  7.7096483 ,  1.96769873,  0.24869976,  0.5086522 ,\n",
       "        0.22879045,  0.42044689,  0.13784036,  0.42030785,  2.61092778,\n",
       "        5.19274721,  0.44184438,  0.27562633,  0.93922432,  7.66714085])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-A.T@z-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218.9917510533116"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = cp.Variable(30)\n",
    "objective = cp.Maximize(-b@z-cp.sum(cp.exp(-A.T@z-1)))\n",
    "prob = cp.Problem(objective,[])\n",
    "prob.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.34340259,  0.10697478, -0.25283001,  0.21451717, -0.51255088,\n",
       "        0.16653594,  0.61571971, -0.29855132, -0.11562343, -0.14247313,\n",
       "        0.24427931,  0.41259437, -0.05106605, -0.0946379 ,  0.23959577,\n",
       "        0.60849946, -0.57510286,  0.27093009,  0.27722523,  0.19888811,\n",
       "       -0.24309424,  0.07629719, -0.77717196, -0.73595885,  0.15500217,\n",
       "        0.4537255 , -0.25244459,  0.00338   ,  0.03396786,  0.31713797])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
