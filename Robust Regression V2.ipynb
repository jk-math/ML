{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Regression\n",
    "\n",
    "$$ \\text{minimize  } \\| Ax-b \\|_1 $$\n",
    "\n",
    "with variable $x$, where $A$ is an $m\\times n$ matrix and $b$ is an $m \\times 1$ vector.\n",
    "\n",
    "Note that the objective function is not differentiable, so we can't compute the gradient or Hessian. So, we can't use Gradient Descent or Newton's Method to fit the model. We can recast the problem as an inequality constrained LP:\n",
    "\n",
    "$$ \\text{minimize  } \\mathbf{1}^Tt \\\\ \\text{subject to  } -t \\preceq Ax-b \\preceq t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve this problem, we recast the problem as an unconstrained minimization problem using the log barrier function.\n",
    "\n",
    "$$ \\text{Log Barrier Function: } \\qquad s \\cdot f_0(x) + \\phi(x) $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\phi(x) = - \\displaystyle\\sum_{i=1}^{\\ell} \\log(-f_i(x)),$$\n",
    "\n",
    "$f_i(x)$ are the $\\ell$ inequality constraint functions, and $f_0(x)$ is the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this application, there are $2m$ inequality constraints. For each $i$, $i=1,2,...,m$\n",
    "\n",
    "$$-t_i \\leq a_i^Tx - b_i \\leq t_i $$\n",
    "\n",
    "where $a_i^T$ is the $i$th row of the matrix $A$. We write this inequality as two constraint functions in standard form:\n",
    "\n",
    "$$ f_i(x,t) = -t_i - a_i^Tx + b_i \\leq 0 \\qquad \\qquad g_i(x,t) = -t_i+a_i^T x-b_i \\leq 0 $$\n",
    "\n",
    "We write the log barrier objective as\n",
    "\n",
    "$$ s \\cdot \\mathbf{1}^Tt - \\displaystyle\\sum_{i=1}^{m} \\log(-f_i(x,t)) - \\displaystyle\\sum_{i=1}^{m} \\log(-g_i(x,t))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some test data\n",
    "m=2000\n",
    "n=4\n",
    "A = np.concatenate([np.random.beta(2,5,(m,1)), np.random.normal(1,0.25, (m,1)), \n",
    "                    np.random.binomial(1,0.7,(m,1)), np.random.uniform(1,2,(m,1)),\n",
    "                   np.ones((m,1))], axis=1) # column of 1's to fit an intercept\n",
    "real_weights = 10*np.random.uniform(-1,1,n+1)\n",
    "b = A@real_weights - np.random.normal(0,1, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find an initial feasible point. We need to find values of $x, t$ such that \n",
    "\n",
    "$$ -f_i(x,t) > 0 \\qquad \\text{and} \\qquad -g_i(x,t) >0$$\n",
    "\n",
    "If we initialize $x$ to be the vector of all zeros, then we need to initialize $t$ so that\n",
    "\n",
    "$$ t_i - b_i > 0 \\qquad \\text{and} \\qquad t_i+b_i > 0 $$\n",
    "\n",
    "for all $i$. We can achieve this by setting every $t_i$ to a value slightly larger than the maximum of the absolute value of all the values in the vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_barrier_objective(x,t,s, A,b):\n",
    "    return s*np.sum(t) - np.sum(np.log(t+A@x-b)) - np.sum(np.log(t-A@x+b))\n",
    "def l1_norm(x):\n",
    "    return np.sum(np.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the barrier method to solve the original problem, we now need to minimize the log barrier function for a given value of $s$. In order to implement Newton's method, we need to know the gradient and Hessian of the log barrier objective.\n",
    "\n",
    "Write the objective as\n",
    "\n",
    "$$ s \\cdot \\mathbf{1}^T t - \\displaystyle\\sum_{i=1}^m \\log((t+Ax-b)_i) - \\displaystyle\\sum_{i=1}^m \\log((t-Ax+b)_i) $$\n",
    "\n",
    "The derivative with respect to the vector $x$ is\n",
    "\n",
    "$$-A^T \\frac{1}{t+Ax-b} + A^T \\frac{1}{t-Ax+b} = A^T \\left(\\frac{-1}{t+Ax-b} +  \\frac{1}{t-Ax+b}\\right)$$\n",
    "\n",
    "and the derivative with respect to $t$ is\n",
    "\n",
    "$$s \\cdot \\mathbf{1} - \\frac{1}{t+Ax-b} - \\frac{1}{t-Ax+b}$$.\n",
    "\n",
    "When we write $\\frac{1}{v}$ where $v$ is a vector, we mean the vector with entries $\\frac{1}{v_i}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "As the number of data points grows ($m$ gets larger), the time it takes to compute the Newton step increases dramatically. So, for a large number of observations, Newton's method will be slow. The subgradient method runs much faster, but the results of the method may vary depending on how the step length is adjusted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x,t,s, A, b):\n",
    "    r = A@x-b\n",
    "    D1 = 1/(t+r)\n",
    "    D2 = 1/(t-r)\n",
    "    return np.append(A.T@(-D1 + D2),s*np.ones(m)  -D1 - D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x,t,s, A, b):\n",
    "    r = A@x-b\n",
    "    D1 = 1/(t+r)**2\n",
    "    D2 = 1/(t-r)**2\n",
    "    tt = np.diag(D1 + D2)\n",
    "    xx = A.T@tt@A\n",
    "    d = np.diag(D1-D2)\n",
    "    tx = d@A\n",
    "    xt = A.T@d\n",
    "    return np.concatenate([np.concatenate([xx,tx]),np.concatenate([xt,tt])],axis=1)\n",
    "def newton_step(x,t,s):\n",
    "    return -1*np.linalg.inv(hessian(x,t,s))@grad(x,t,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking line search\n",
    "def backtrack(objective, x, t, s, A, b, grad, descent_direction, alpha, beta):\n",
    "    t0 = 1\n",
    "    # we need to make sure that the input is in the domain\n",
    "    while np.min(t+t0*descent_direction[n+1:]+A@(x+t0*descent_direction[:n+1])-b)<=0 or np.min(t+t0*descent_direction[n+1:]-A@(x+t0*descent_direction[:n+1])+b)<=0:\n",
    "        t0 = t0*beta\n",
    "    while objective(x+t0*descent_direction[:n+1],t+t0*descent_direction[n+1:],s,A,b) > objective(x,t,s,A,b)+ alpha*t0*grad@descent_direction:\n",
    "        t0 = t0*beta\n",
    "    return t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feasible starting point\n",
    "x = np.zeros(n+1)\n",
    "t = np.array(m*[np.max(np.abs(b))+0.1])\n",
    "# some descent parameters\n",
    "tol = 1e-7\n",
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "max_iter = 100 # for centering step\n",
    "s = 1\n",
    "mu = 100\n",
    "num_inequalities = 2*m\n",
    "while num_inequalities/s >= tol:\n",
    "    i=0\n",
    "    # centering step\n",
    "    while i <= max_iter:\n",
    "        g = grad(x,t,s,A,b)\n",
    "        H = hessian(x,t,s,A,b)\n",
    "        newton_step = -np.linalg.inv(H)@g\n",
    "        dec = -g@newton_step\n",
    "        if dec < tol:\n",
    "            break\n",
    "        else:\n",
    "            step_length = backtrack(log_barrier_objective,x,t, s,A,b,g, newton_step,alpha,beta)\n",
    "            x = x+step_length*newton_step[:n+1]\n",
    "            t = t+step_length*newton_step[n+1:]\n",
    "        i +=1\n",
    "    s = s*mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.04327543,  0.39263291,  6.37471936, -8.32941967, -3.64331753])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.24774134,  0.51859918,  6.32177035, -8.28164707, -3.68701098])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582.584198027401"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm(A@x-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582.5841974210807"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Problem\n",
    "z = cp.Variable(n+1)\n",
    "objective = cp.Minimize(cp.norm(A@z-b, 1))\n",
    "prob = cp.Problem(objective, [])\n",
    "prob.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.04327543,  0.39263291,  6.37471936, -8.32941967, -3.64331753])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgradient Method\n",
    "\n",
    "Newton's Method is slow when the data matrix $A$ is large. A faster method is the subgradient method.\n",
    "\n",
    "A subgradient of $\\| Ax - b \\|_1$ is\n",
    "\n",
    "$$ A^T\\text{sign}(Ax-b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgrad(x,A,b):\n",
    "    return A.T@np.sign(A@x-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.zeros(n+1)\n",
    "max_iters = 10000\n",
    "step_length = 0.1\n",
    "best_val = l1_norm(A@x-b)\n",
    "best_x = np.zeros(n+1)\n",
    "i = 0\n",
    "while i <= max_iters:\n",
    "    x = x - step_length*subgrad(x,A,b)\n",
    "    obj_val = l1_norm(A@x-b)\n",
    "    if obj_val < best_val:\n",
    "        best_x = x\n",
    "        best_val = obj_val\n",
    "    i += 1\n",
    "    step_length = 0.1/(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.04276794,  0.39299489,  6.37501349, -8.32843116, -3.64540557])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1582.5843954231673"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
