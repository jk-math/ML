{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson Regression\n",
    "\n",
    "This is an example of an application of Convex Optimization found in the [Convex Optimization textbook](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) by Boyd and Vandenberghe. There is a great [online course](https://www.edx.org/course/convex-optimization) offered on EdX which I highly recommend. \n",
    "\n",
    "Suppose $Y$ is a Poisson random variable with mean $\\mu$:\n",
    "\n",
    "$$ P(Y=k) = \\frac{\\mu^k e^{-k}}{k!} $$\n",
    "\n",
    "Suppose the mean $\\mu$ is a linear combination of some explanatory variables:\n",
    "\n",
    "$$ \\mu = a^Tx + b$$\n",
    "\n",
    "and that we have some data $(x_i,y_i)$ for $i=1,2,...,m$ where $x_i \\in \\mathbb{R}^n$ is a vector of explanatory variable values and $y_i$ is the response variable. We want to find MLE's of the parameters $a$ and $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood is\n",
    "\n",
    "$$ \\displaystyle\\sum_{i=1}^m\\left( y_i\\log(a^Tx_i+b) - (a^Tx_i+b) - \\log(y_i!) \\right)$$\n",
    "\n",
    "Instead of maximizing the above log likelihood, we will solve the problem of minimizing \n",
    "\n",
    "$$ \\displaystyle\\sum_{i=1}^m a^Tx_i+b - y_i\\log(a^Tx_i+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compile all of the observed data into a single design matrix, where we also include a column of ones for the intercept $b$. We wish to minimize\n",
    "\n",
    "$$ \\displaystyle\\sum_{i=1}^m \\left[(Xz)_i - y_i \\log((Xz)_i) \\right] $$\n",
    "\n",
    "where $X$ is the matrix whose rows are $x_1, x_2, ..., x_m$, with an additional column of ones for the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In many other textbooks, Poisson regression is stated as follows. \n",
    "\n",
    "$$E\\left[Y|x\\right] = \\mu \\qquad \\text{ and } \\qquad \\log(\\mu) = Xz$$\n",
    "\n",
    "where $z$ is again the parameter vector we wish to estimate. So the assumption is the log of the expected value is a linear function of the parameter vector $z$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate test data\n",
    "n = 4 # number of explanatory variables\n",
    "m = 20000 # number of data points\n",
    "real_z = np.append(10*np.random.uniform(-1,1,n), 50) # last entry of z represents the intercept\n",
    "# design matrix\n",
    "X = np.concatenate([np.random.beta(2,5,(m,1)), np.random.normal(1,0.25, (m,1)), \n",
    "                    np.random.binomial(1,0.7,(m,1)), np.random.uniform(1,2,(m,1)),\n",
    "                   np.ones((m,1))], axis=1)\n",
    "real_mu = X@real_z\n",
    "y = np.random.poisson(real_mu, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.26866953,  2.37727535,  7.84591618, -6.20268071, 50.        ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(X,y,z):\n",
    "    return np.sum((X@z)-y*np.log(X@z))\n",
    "def grad(X,y,z):\n",
    "    return X.T@(1 -y/(X@z))\n",
    "def backtrack(obj,X,y,z, gradient, alpha, beta):\n",
    "    t = 1\n",
    "    while np.min(X@(z-t*gradient)) <= 0 :\n",
    "        t = t*beta\n",
    "    while obj(X,y,z - t*gradient) >  obj(X,y,z) - alpha*t*gradient@gradient:\n",
    "        t = t*beta\n",
    "    return t\n",
    "def l2_norm(x):\n",
    "    return np.sqrt(np.sum(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "z = np.append(np.zeros(n),0.1) # initial point\n",
    "tol = 1e-8 # stopping criterion tolerance for L2 norm of gradient\n",
    "max_iter = 1000\n",
    "# parameters \n",
    "alpha = 0.1 # line search\n",
    "beta = 0.5 # line search\n",
    "i = 0\n",
    "while i <= max_iter:\n",
    "    g = grad(X,y,z)\n",
    "    if l2_norm(g) < tol:\n",
    "        break\n",
    "    else:\n",
    "        t = backtrack(nll, X, y, z, g, alpha, beta) # step length\n",
    "        z = z - t*g # update\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.18370646,  2.30877419,  7.98245933, -6.06223243, 49.70311673])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(X,y,z):\n",
    "    return X.T@np.diag(y/((X@z)**2))@X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NM_backtrack(obj,X,y,z, gradient, newton_step, alpha, beta):\n",
    "    t = 1\n",
    "    while np.min(X@(z+t*newton_step)) <= 0 :\n",
    "        t = t*beta\n",
    "    while obj(X,y,z+t*newton_step) >  obj(X,y,z) + alpha*t*gradient@newton_step:\n",
    "        t = t*beta\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method\n",
    "z = np.append(np.zeros(n),0.1)\n",
    "tol = 1e-8 # stopping criterion tolerance for L2 norm of gradient\n",
    "max_iter = 100\n",
    "# parameters \n",
    "alpha = 0.1 # line search\n",
    "beta = 0.8 # line search\n",
    "i = 0\n",
    "while i <= max_iter:\n",
    "    g = grad(X,y,z)\n",
    "    H = hessian(X,y,z)\n",
    "    newton_step = -np.linalg.inv(H)@g\n",
    "    dec = -g@newton_step\n",
    "    if dec < tol:\n",
    "        break\n",
    "    else:\n",
    "        t = NM_backtrack(nll, X, y,z, g, newton_step, alpha, beta)\n",
    "        z = z+t*newton_step\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.21260883,  2.28618061,  7.97928613, -6.08617612, 49.77380631])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 95% confidence interval for the 0 th entry of the paramter vector is -3.2253 to 0.8001\n",
      "A 95% confidence interval for the 1 th entry of the paramter vector is 0.278 to 4.2944\n",
      "A 95% confidence interval for the 2 th entry of the paramter vector is 5.9747 to 9.9839\n",
      "A 95% confidence interval for the 3 th entry of the paramter vector is -8.0933 to -4.0791\n",
      "A 95% confidence interval for the 4 th entry of the paramter vector is 47.7593 to 51.7883\n"
     ]
    }
   ],
   "source": [
    "# confidence interval for parameter vector z\n",
    "covar = np.linalg.inv(X.T@np.diag(X@z)@X)\n",
    "lower_end = z -  2*np.exp(np.sqrt(np.diag(covar)))\n",
    "upper_end = z +  2*np.exp(np.sqrt(np.diag(covar)))\n",
    "for i in range(len(lower_end)):\n",
    "    print('A 95% confidence interval for the', i,'th entry of the paramter vector is', round(lower_end[i],4), 'to', round(upper_end[i],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
