{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chebyshev or Minimax Approximation\n",
    "\n",
    "$$ \\text{minimize  } \\| Ax-b \\|_{\\infty} $$\n",
    "\n",
    "where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$ are given.\n",
    "\n",
    "We can recast this problem as an LP. Consider the epigraph form of the problem.\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimize  } & \\qquad t\\\\\n",
    "\\text{subject to  } & \\qquad \\| Ax-b \\|_{\\infty} \\leq t\n",
    "\\end{align*}\n",
    "\n",
    "The constraints imply that $|a_i^Tx - b_i| \\leq t$ for all $1\\leq i \\leq m$, where $a_i^T$ is the $i^{\\text{th}}$ row of $A$. Hence,\n",
    "\n",
    "$$ -t \\leq a_i^Tx - b_i \\leq t \\qquad \\text{ for all  } 1\\leq i \\leq m$$\n",
    "\n",
    "which we can write as\n",
    "\n",
    "$$ -t\\mathbf{1} \\preceq Ax - b \\preceq t\\mathbf{1} $$\n",
    "\n",
    "Thus, we recast the original problem as\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimize  } & \\qquad t\\\\\n",
    "\\text{subject to  } & \\qquad -t\\mathbf{1} \\preceq Ax-b  \\preceq t\\mathbf{1}\n",
    "\\end{align*}\n",
    "\n",
    "Once we solve this inequality constrained problem with variables $t$ and $x$, the optimal value of $t$ will be the optimal value of the original problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the log barrier method to solve this LP. For increasing values of $s$, we will minimize the objective\n",
    "\n",
    "$$ st - \\displaystyle\\sum_{i=1}^m \\log\\left((t\\mathbf{1}+Ax-b)_i\\right) -\\displaystyle\\sum_{i=1}^m \\log\\left((t\\mathbf{1}-Ax+b)_i\\right)$$\n",
    "\n",
    "We will work with a single variable $\\mathbf{w} = \\begin{bmatrix} t \\\\ \\mathbf{x} \\end{bmatrix}$. We compute the gradient of the objective. The derivative with respect to $t$ is\n",
    "\n",
    "$$ s -  \\displaystyle\\sum_{i=1}^m \\frac{1}{(t\\mathbf{1}+Ax-b)_i} - \\displaystyle\\sum_{i=1}^m \\frac{1}{(t\\mathbf{1}-Ax+b)_i}$$\n",
    "\n",
    "and the derivative with respect to $\\mathbf{x}$ is\n",
    "\n",
    "$$ -A^T \\left( \\frac{1}{t\\mathbf{1}+Ax-b} - \\frac{1}{t\\mathbf{1}-Ax+b} \\right) $$\n",
    "\n",
    "where we use $\\frac{1}{t\\mathbf{1}+Ax-b}$ to indicate the vector with $i^{\\text{th}}$ entry $\\frac{1}{t+a_i^Tx-b_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "m=200\n",
    "n=4\n",
    "A = np.concatenate([np.random.beta(2,5,(m,1)), np.random.normal(1,0.25, (m,1)), \n",
    "                    np.random.binomial(1,0.7,(m,1)), np.random.uniform(1,2,(m,1))], axis=1)\n",
    "real_weights = 10*np.random.uniform(-1,1,n)\n",
    "b = A@real_weights + np.random.normal(0,1,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.83623256,  7.34841271,  5.51589012, -9.88421998])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3420471080598126"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Problem\n",
    "z = cp.Variable(n)\n",
    "objective = cp.Minimize(cp.norm(A@z-b, 'inf'))\n",
    "prob = cp.Problem(objective, [])\n",
    "prob.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64154898,  8.38109763,  4.99354034, -9.84817105])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3420471080598126"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem recast as LP\n",
    "v = cp.Variable(n+1)\n",
    "objective = cp.Minimize(v[0]) # first entry of v is the value t from above; it is the value of the objective function\n",
    "constraints = [-v[0]-A@v[1:]+b <=0,-v[0]+A@v[1:]-b <=0 ]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "prob.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34204711,  0.64154898,  8.38109763,  4.99354034, -9.84817105])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the variable $w$ as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_norm(x):\n",
    "    return np.max(np.abs(x))\n",
    "def l2_norm(x):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "# objective plus log barrier, where s is parameter we will increase\n",
    "def f(w, s, A, b):\n",
    "    return s*w[0] - np.sum(np.log(w[0]*np.ones(m)+A@w[1:]-b)) - np.sum(np.log(w[0]*np.ones(m)-A@w[1:]+b))\n",
    "def grad(w, s, A, b):\n",
    "    D1 = w[0]*np.ones(m)+A@w[1:]-b\n",
    "    D2 = w[0]*np.ones(m)-A@w[1:]+b\n",
    "    grad_t = s - np.sum(1/D1)-np.sum(1/D2)\n",
    "    grad_x = -A.T@(1/D1-1/D2)\n",
    "    return np.append(grad_t,grad_x)\n",
    "# backtracking line search used for each centering step\n",
    "def backtrack(objective, w, s, A,b, alpha, beta, grad, descent_direction):\n",
    "    t0 = 1\n",
    "    # since we are dealing with logs, must verify that updated point lands in domain of log\n",
    "    while ((w[0]+t0*descent_direction[0])*np.ones(m)+A@(w[1:]+t0*descent_direction[1:])-b <= 0).any() or ((w[0]+t0*descent_direction[0])*np.ones(m)-A@(w[1:]+t0*descent_direction[1:])+b <= 0).any():\n",
    "        t0 = beta*t0\n",
    "    while objective(w+t0*descent_direction,s,A,b) > objective(w,s,A,b)+t0*alpha*grad@descent_direction:\n",
    "        t0 = beta*t0\n",
    "    return t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(w, A, b):\n",
    "    D1 = w[0]*np.ones(m)+A@w[1:]-b\n",
    "    D2 = w[0]*np.ones(m)-A@w[1:]+b\n",
    "    wrt_t = np.append(np.sum(1/D1**2 +1 /D2**2),-A.T@(1/D1**2 - 1 /D2**2)).reshape(n+1,1)\n",
    "    tx = (1/D1**2-1/D2**2)@A\n",
    "    tx = tx.reshape(1,n)\n",
    "    xx = A.T@np.diag(1/D1**2+1/D2**2)@A\n",
    "    \n",
    "    return np.concatenate([wrt_t,np.concatenate([tx,xx])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for centering step\n",
    "# this does not run quickly\n",
    "tol = 1e-5\n",
    "alpha = 0.01\n",
    "beta = 0.5\n",
    "max_iter = 1000\n",
    "i = 0\n",
    "w = np.append(np.max(np.abs(b))+0.1,np.zeros(n)) # feasible initial point\n",
    "s = 1\n",
    "mu = 5\n",
    "num_inequalities = 2*m\n",
    "while num_inequalities/s >= tol:\n",
    "    i=0\n",
    "    # centering step\n",
    "    while i <= max_iter:\n",
    "        g = grad(w,s,A,b)\n",
    "        if l2_norm(g) < tol:\n",
    "            break\n",
    "        else:\n",
    "            step_length = backtrack(f,w,s, A,b,alpha,beta, g, -g)\n",
    "            w = w - step_length*g\n",
    "        i +=1\n",
    "    s = s*mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.44946621,  1.36014089,  7.95729658,  5.48571401, -9.95831392])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method\n",
    "tol = 1e-7\n",
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "max_iter = 50\n",
    "i = 0\n",
    "w = np.append(np.max(np.abs(b))+0.1,np.zeros(n)) # feasible initial point\n",
    "s = 1\n",
    "mu = 2\n",
    "num_inequalities = 2*m\n",
    "while num_inequalities/s >= tol:\n",
    "    i=0\n",
    "    dec = np.inf\n",
    "    # centering step\n",
    "    while i <= max_iter:\n",
    "        g = grad(w,s,A,b)\n",
    "        H = hessian(w,A,b)\n",
    "        newton_step = -np.linalg.inv(H)@g\n",
    "        dec = -g@newton_step\n",
    "        if dec < tol:\n",
    "            break\n",
    "        else:\n",
    "            step_length = backtrack(f,w,s, A,b,alpha,beta, g, newton_step)\n",
    "            w = w+step_length*newton_step\n",
    "        i +=1\n",
    "    s = s*mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.34204711,  0.64154929,  8.38109755,  4.99354035, -9.84817102])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
